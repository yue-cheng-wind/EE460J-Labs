{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soroush Famili, James Lu, Nithanth Ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Revisiting Logistic Regression and MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 5, you solved the handwriting recognition problem (the MNIST data set) using multi-class Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Random Forests to try to get the best possible test accuracy on MNIST. This involves\n",
    "getting acquainted with how Random Forests work, understanding their parameters, and\n",
    "therefore using Cross Validation to find the best settings. How well can you do? You should\n",
    "use the accuracy metric, since this is what you used in Lab 5 â€“ therefore this will allow you to\n",
    "compare your results from Random Forests with your results from L1- and L2- Regularized\n",
    "Logistic Regression. What are the hyperparameters of your best model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[x for x in range(10, 80, 10)],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_features='log2', max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes approximately 16 minutes to run\n",
    "forest_CV = GridSearchCV(estimator=forest, param_grid=param_grid, cv=10)\n",
    "forest_CV.fit(X_train, y_train)\n",
    "print(forest_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9621428571428572\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    predictions.append(forest_CV.predict([X_test[i, :]]))\n",
    "    \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Boosting to do the same. Take the time to understand how XGBoost works (and/or\n",
    "other boosting packages available). Try your best to tune your hyper-parameters. As added\n",
    "motivation: typically the winners and near-winners of the Kaggle competition are those that\n",
    "are best able to tune an cross validate XGBoost. What are the hyperparameters of your best\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "forest2 = xgb.XGBClassifier(n_jobs=-1)\n",
    "forest2.fit(X_train, y_train)\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_Boost = {\n",
    "    'max_depth':[x for x in range(2,4)],\n",
    "    'min_child_weight':[x for x in range(1,4)]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "forest_GS = GridSearchCV(estimator=forest2, param_grid=param_grid_Boost, cv=5)\n",
    "forest_GS.fit(X_train, y_train)\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forest_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
