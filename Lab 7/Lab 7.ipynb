{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soroush Famili, James Lu, Nithanth Ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Revisiting Logistic Regression and MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Lab 5, you solved the handwriting recognition problem (the MNIST data set) using multi-class Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Random Forests to try to get the best possible test accuracy on MNIST. This involves getting acquainted with how Random Forests work, understanding their parameters, and therefore using Cross Validation to find the best settings. How well can you do? You should use the accuracy metric, since this is what you used in Lab 5 â€“ therefore this will allow you to compare your results from Random Forests with your results from L1- and L2- Regularized Logistic Regression. What are the hyperparameters of your best model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5792430559794108 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[x for x in range(10, 80, 10)],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_features='log2', max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes approximately 16 minutes to run\n",
    "forest_CV = GridSearchCV(estimator=forest, param_grid=param_grid, cv=10)\n",
    "forest_CV.fit(X_train, y_train)\n",
    "print(forest_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    predictions.append(forest_CV.predict([X_test[i, :]]))\n",
    "    \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use Boosting to do the same. Take the time to understand how XGBoost works (and/or other boosting packages available). Try your best to tune your hyper-parameters. As added motivation: typically the winners and near-winners of the Kaggle competition are those that are best able to tune an cross validate XGBoost. What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest2 = xgb.XGBClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_Boost = {\n",
    "    'n_estimators':[x for x in range(10,40,10)],\n",
    "    'max_depth':[x for x in range(1,5)],\n",
    "    'learning_rate':[.05],\n",
    "    'subsample':[.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "forest_GS = GridSearchCV(estimator=forest2, param_grid=param_grid_Boost, cv=5)\n",
    "forest_GS.fit(X_train, y_train, eval_metric='auc')\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))\n",
    "print(forest_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, forest_GS.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Revisiting Logistic Regression and CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now  that  you  have  your  pipeline  set  up,  it  should  be  easy  to  apply  the  above  procedure  toCIFAR-10. If you did something that takes significant computation time, keep in mind that CIFAR-10 is a few times larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the best accuracy you can get on the test data, by tuning Random Forests?  What are the hyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X, y = fetch_openml('cifar-10-small', return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators':[x for x in range(10, 80, 10)],\n",
    "    'criterion':['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_features='log2', max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes approximately 16 minutes to run\n",
    "forest_CV = GridSearchCV(estimator=forest, param_grid=param_grid, cv=10)\n",
    "forest_CV.fit(X_train, y_train)\n",
    "print(forest_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    predictions.append(forest_CV.predict([X_test[i, :]]))\n",
    "    \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the best accuracy you can get on the test data, by tuning XGBoost?  What are thehyperparameters of your best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest2 = xgb.XGBClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_Boost = {\n",
    "    'n_estimators':[x for x in range(10,40,10)],\n",
    "    'max_depth':[x for x in range(1,5)],\n",
    "    'learning_rate':[.05],\n",
    "    'subsample':[.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "forest_GS = GridSearchCV(estimator=forest2, param_grid=param_grid_Boost, cv=5)\n",
    "forest_GS.fit(X_train, y_train, eval_metric='auc')\n",
    "print(\"%s minutes\" % ((time.time() - start_time)/60))\n",
    "print(forest_GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, forest_GS.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
